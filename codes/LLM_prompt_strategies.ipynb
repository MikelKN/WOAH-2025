{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Wwz_5omNZD4VO9x6r-QnCeJvnukzIpVI",
      "authorship_tag": "ABX9TyN4ymyk6W1T1CdJx5OccLjO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "49e27c57401d488fb3fb14991ca6e386": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4892aa43c30645dba05dc3d5ca4259b0",
              "IPY_MODEL_e4de42ebf52747e6be1f9eafaba80fbd",
              "IPY_MODEL_ddadb21308e448e4951f411cc31132b1"
            ],
            "layout": "IPY_MODEL_4838e4c35d6046c5afdee6be43e2b5bf"
          }
        },
        "4892aa43c30645dba05dc3d5ca4259b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7ee387cfabe47608c5c3773b5246408",
            "placeholder": "​",
            "style": "IPY_MODEL_2847ff752e4442dbaac5f51b54a3cbfd",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "e4de42ebf52747e6be1f9eafaba80fbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88267298ee314c07aae871c0fce28c48",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1cd983545b8f45a8a600340c2450f9be",
            "value": 3
          }
        },
        "ddadb21308e448e4951f411cc31132b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d8498a996aa4debbe845fcfec645819",
            "placeholder": "​",
            "style": "IPY_MODEL_fc3d1773dd404ef7bb24f549b4568073",
            "value": " 3/3 [01:13&lt;00:00, 24.20s/it]"
          }
        },
        "4838e4c35d6046c5afdee6be43e2b5bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7ee387cfabe47608c5c3773b5246408": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2847ff752e4442dbaac5f51b54a3cbfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88267298ee314c07aae871c0fce28c48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cd983545b8f45a8a600340c2450f9be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9d8498a996aa4debbe845fcfec645819": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc3d1773dd404ef7bb24f549b4568073": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MikelKN/WOAH-2025/blob/main/LLM_prompt_strategies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ALL the PROMPTS and STRATEGIES"
      ],
      "metadata": {
        "id": "P0z5ymXdcm0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# limit to the lenght of token that should be displayed on the daatsets\n",
        "!pip install openai\n",
        "from openai import OpenAI\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Class for Cohere AI\n",
        "!pip install cohere transformers --quiet\n",
        "!pip install openai mistral_inference llamaapi>=0.1.36 tqdm fire --quiet\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# import kaggle\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import cohere\n",
        "\n",
        "pd.options.display.max_colwidth = 500\n",
        "\n",
        "from pathlib import Path\n",
        "# !pip install openai\n",
        "from openai import OpenAI\n",
        "\n",
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ[\"COHERE_PROD\"] = userdata.get(\"COHERE_PROD\")\n",
        "os.environ[\"NEW_OPEN_AI\"] = userdata.get(\"NEW_OPEN_AI\")\n",
        "# os.environ[\"OPEN_AI\"] = userdata.get(\"OPEN_AI\")\n",
        "\n",
        "\n",
        "class Cohere_ai:\n",
        "    def prompt_cohere(self, event, model_name= 'command-r7b-12-2024'):\n",
        "      prompt_empathy_vanilla = f\"Assume the role of an NGO professional specializing in countering online hate speech with empathy and fostering constructive discourse.\\\n",
        "      Your task is to generate a concise, well-reasoned, and compassionate counter-narrative in response to the following comment: {event}. \\\n",
        "      Your responses should closely mirror the knowledge and abilities of an NGO worker. No additional explanations are required. provide the counter narrative only.\"\n",
        "      co_api = cohere.Client(COHERE_PROD)\n",
        "      response = co_api.chat(\n",
        "          message=prompt_empathy_vanilla,\n",
        "          model=model_name,\n",
        "          temperature=0.3\n",
        "      )\n",
        "      return response.text\n",
        "\n",
        "class gpt_4o_model:\n",
        "    def prompt_gpt4o(self, event, model_name = 'gpt-4o-mini'):\n",
        "      prompt_empathy_ngo = f\"Assume the role of an NGO professional specializing in countering online hate speech with empathy and fostering constructive discourse.\\\n",
        "      Your task is to generate a concise, well-reasoned, and compassionate counter-narrative in response to the following comment: {event}. \\\n",
        "      Your responses should closely mirror the knowledge and abilities of an NGO worker. No additional explanations are required. provide the counter narrative only.\"\n",
        "      client = OpenAI(\n",
        "          api_key = NEW_OPEN_AI\n",
        "          )\n",
        "      input = [{\"role\": \"system\", \"content\": \"You are a NGO worker and expert in generating compasssionate countrer narrative.\"}, #vanilla : \"You are a helpful assistant.\" \"You are an ngo worker on a mission to mitigate hateful language online.\"\n",
        "              {\"role\": \"user\",\"content\": prompt_empathy_ngo,}]\n",
        "\n",
        "      chat_completion = client.chat.completions.create(\n",
        "          messages=input,\n",
        "          model=model_name,\n",
        "          temperature=0.3\n",
        "      )\n",
        "      output = chat_completion.choices[0].message.content\n",
        "      return output\n",
        "\n",
        "class llama_instruct:\n",
        "    def prompt_llama(self, event, model_name =\"llama3.1-70b\"):\n",
        "      prompt = f\"Assume the role of an NGO professional specializing in countering online hate speech with empathy and fostering constructive discourse.\\\n",
        "      Your task is to generate a concise, well-reasoned, and compassionate counter-narrative in response to the following comment: {event}. \\\n",
        "      Your responses should closely mirror the knowledge and abilities of an NGO worker. No additional explanations are required. provide the counter narrative only.\"\n",
        "\n",
        "      client = OpenAI(\n",
        "          api_key = LLama-API-Key,\n",
        "          base_url = \"https://api.llama-api.com\")\n",
        "\n",
        "      input = [{\"role\": \"system\", \"content\": \"You are an NGO worker and expert in generating compasssionate countrer narrative.\" }, #You are a counter narrative expert.\"You are an ngo worker on a mission to mitigate hateful language online.\"\n",
        "              {\"role\": \"user\",\"content\": prompt,}]\n",
        "\n",
        "      chat_completion = client.chat.completions.create(\n",
        "      model=model_name,\n",
        "      messages=input,\n",
        "      temperature = 0.3\n",
        "      )\n",
        "      output = chat_completion.choices[0].message.content\n",
        "\n",
        "      return output"
      ],
      "metadata": {
        "id": "oe1LRWQid3cz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39b68580-328d-41a3-cc42-4d40c037e87e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.9/253.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment and emotion label"
      ],
      "metadata": {
        "id": "-oQhZBWqyVi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.kaggle.com/code/ncho11/introduction-to-prompt-engineering-using-mistral\n",
        "\n",
        "!pip install transformers --quiet\n",
        "!pip install accelerate --quiet\n",
        "!pip install bitsandbytes --quiet\n",
        "!pip install langchain --quiet\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore') # avoid warning messages importing packages\n",
        "\n",
        "os.environ[\"HF_API_KEY\"] = userdata.get(\"HF_API_KEY\")\n",
        "os.environ[\"COHERE_API\"] = userdata.get(\"COHERE_API\")\n",
        "\n",
        "HF_TOKEN = userdata.get(\"HF_API_KEY\")\n",
        "from huggingface_hub import login\n",
        "login(HF_TOKEN)\n",
        "\n",
        "import torch\n",
        "import re\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "\n",
        "mistral_model = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(mistral_model, use_fast= True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "mistral_model_new = AutoModelForCausalLM.from_pretrained(mistral_model,\n",
        "                                                         torch_dtype=torch.float16,\n",
        "                                                         trust_remote_code=True,\n",
        "                                                         device_map=\"auto\",\n",
        "                                                         quantization_config=quantization_config)\n",
        "\n",
        "generation_config = GenerationConfig.from_pretrained(mistral_model)\n",
        "generation_config.max_new_tokens = 20\n",
        "generation_config.temperature = 0.1\n",
        "generation_config.do_sample = True\n",
        "\n",
        "mistral_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=mistral_model_new,\n",
        "    tokenizer=tokenizer,\n",
        "    truncation=True,\n",
        "    return_full_text=False,\n",
        "    generation_config=generation_config,\n",
        ")\n",
        "output_path= 'some path'\n",
        "\n",
        "class LLM_sentiment:\n",
        "\n",
        "    def sentiment_mistral_2(self, event):\n",
        "        prompt = f\"\"\"\n",
        "        As an expert in sentiment analysis, analyze the following '{event}' and determine the dominant sentiment it evokes.\n",
        "        Reply with strictly one sentiment(lowercase, no quotes, and no punctuations) chosen from: very negative, negative, neutral, positive, or very positive.\n",
        "        Response should be in the following structured format (strictly one line):\n",
        "        sentiment: chosen sentiment\n",
        "        Ensure that the response strictly follows this format, with no additional explanation, commentary, or justification beyond the required output. No other response will be accepted.\n",
        "        \"\"\"\n",
        "        response = mistral_pipeline(prompt)\n",
        "        output = response[0]['generated_text'].strip()\n",
        "        # print(f\"Model Output:\\n{output}\\n\")\n",
        "        # return output\n",
        "        try:\n",
        "            lines = [line.strip() for line in output.split(\"\\n\") if line.strip()]\n",
        "            if not lines:\n",
        "                raise ValueError(\"No valid sentiment line found\")\n",
        "            sentiment_line = lines[0]\n",
        "            if sentiment_line.startswith(\"sentiment:\"):\n",
        "                sentiment = sentiment_line.split(\":\", 1)[1].strip().lower()\n",
        "            else:\n",
        "                match = re.search(r\"sentiment:\\s*([a-zA-Z]+)\", lines[-1], re.IGNORECASE)\n",
        "                if not match:\n",
        "                    raise ValueError(\"Response format is incorrect\")\n",
        "                sentiment = match.group(1)\n",
        "            sentiment = re.sub(r\"[^\\w\\s]\", \"\", sentiment).strip().split(' ')[0]\n",
        "            # if not sentiment_line.startswith(\"sentiment:\"):\n",
        "            #     raise ValueError(\"Response format is incorrect\")\n",
        "            # sentiment = sentiment_line.split(\":\", 1)[1].strip().lower()\n",
        "            # sentiment = re.sub(r\"[^\\w\\s]\", \"\", sentiment).strip()\n",
        "            valid_sentiments = {\"very negative\", \"negative\", \"neutral\", \"positive\", \"very positive\"}\n",
        "            if sentiment not in valid_sentiments:\n",
        "                raise ValueError(f\"Invalid sentiment: {sentiment}\")\n",
        "            return {\"sentiment\": sentiment}\n",
        "        except (IndexError, ValueError) as e:\n",
        "            print(f\"Error processing response: {output} | {e}\")\n",
        "            return {\"sentiment\": \"unknown\"}\n",
        "\n",
        "    def emotion_mistral_2(self, event):\n",
        "        prompt = f\"\"\"\n",
        "        You are an expert in psychological and affective computing, analyze the following '{event}' and determine the dominant emotion it evokes.\n",
        "        Reply with strictly one emotion(lowercased, with no quotes, and no punctuations) chosen from this list: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise, or neutral.\n",
        "        Response should be in the following structured format (strictly one line):\n",
        "        emotion: chosen emotion\n",
        "        Ensure that the response strictly follows this format. Do not Include any additional explanation, commentary, or justification.\n",
        "        Do not generate lists, or multiple emotions, or synonyms. No other response will be accepted.\"\"\"\n",
        "        response = mistral_pipeline(prompt)\n",
        "        output = response[0]['generated_text'].strip()\n",
        "        print(f\"\\nModel Output:\\n{output}\\n\")\n",
        "        try:\n",
        "            lines = [line.strip() for line in output.split(\"\\n\") if line.strip()]\n",
        "            if not lines:\n",
        "                raise ValueError(\"No valid sentiment line found\")\n",
        "            emotion_line = lines[0]\n",
        "            if emotion_line.startswith(\"emotion:\"):\n",
        "                emotion = emotion_line.split(\":\", 1)[1].strip().lower()\n",
        "            else:\n",
        "                match = re.search(r\"sentiment:\\s*([a-zA-Z]+(?:\\s[a-zA-Z]+)?)\", lines[-1], re.IGNORECASE)\n",
        "                if not match:\n",
        "                    raise ValueError(\"Response format is incorrect\")\n",
        "                emotion = match.group(1)\n",
        "            emotion = re.sub(r\"[^\\w\\s]\", \"\", emotion).strip().split(' ')[0] #remove punctuation\n",
        "\n",
        "            valid_emotions = {\"admiration\", \"amusement\", \"anger\", \"annoyance\", \"approval\", \"caring\",\n",
        "                              \"confusion\", \"curiosity\", \"desire\", \"disappointment\", \"disapproval\", \"disgust\",\n",
        "                              \"embarrassment\", \"excitement\", \"fear\", \"gratitude\", \"grief\", \"joy\", \"love\",\n",
        "                              \"nervousness\", \"optimism\", \"pride\", \"realization\", \"relief\", \"remorse\", \"sadness\",\n",
        "                              \"surprise\", \"neutral\"}\n",
        "            emotion_mapping = {\n",
        "            \"anxiety\": \"nervousness\",\n",
        "            \"urgency\": \"fear\",\n",
        "            \"acceptance\": 'approval',\n",
        "            \"concern\": \"fear\",\n",
        "            \"nostalgia\": \"realization\",\n",
        "            \"unease\": \"nervousness\",\n",
        "            \"dismay\": \"disappointment\",\n",
        "            \"shock\": \"surprise\",\n",
        "            \"resignation\": \"sadness\",\n",
        "            \"empathy\": \"caring\",\n",
        "            \"compassion\": \"caring\",\n",
        "            \"compassionate\": \"caring\",\n",
        "            \"understanding\": \"caring\",\n",
        "            'appreciation' : \"gratitude\",\n",
        "                \"respect\" : \"admiration\",\n",
        "            \"determination\": \"optimism\"}\n",
        "            if emotion not in valid_emotions:\n",
        "                if emotion in emotion_mapping:\n",
        "                    mapped_emotion = emotion_mapping[emotion]\n",
        "                    print(f\"for Mapping '{emotion}' to standardized label: '{mapped_emotion}'\")\n",
        "                    emotion = mapped_emotion\n",
        "                else:\n",
        "                    raise ValueError(f\"Invalid emotion: {emotion}\")\n",
        "\n",
        "            return {\"emotion\": emotion}\n",
        "\n",
        "        except (IndexError, ValueError) as e:\n",
        "            print(f\"Error processing response: {output} | {e}\")\n",
        "            return {\"emotion\": \"unknown\"}\n",
        "\n",
        "hate_eval_df = pd.read_csv('hate_eval_dataset')\n",
        "conan_df = pd.read_csv('conan_dataset')\n",
        "\n",
        "def main():\n",
        "    llm_prompt = LLM_sentiment()\n",
        "    dataset_mistral = pd.read_csv('/content/drive/MyDrive/conan/conan_all_prompt_gpt_sent_emot.csv')\n",
        "    tqdm.pandas()\n",
        "    #example\n",
        "    dataset_mistral['gpt_ngoemotion_sentiment_mistral'] = dataset['hateeval_gpt40mini_ngoemotion_prompt'].progress_apply(lambda x: pd.Series(llm_prompt.sentiment_mistral_2(x)))\n",
        "    dataset_mistral['gpt_ngoemotion_emotion_mistral'] = dataset['hateeval_gpt40mini_ngoemotion_prompt'].progress_apply(lambda x: pd.Series(llm_prompt.emotion_mistral_2(x)))\n",
        "\n",
        "    file_name = f'hateeval_gpt_ngoemotion_mistral_sentemot_df.csv'\n",
        "    file_path = os.path.join(output_path , file_name)\n",
        "    dataset_mistral.to_csv(file_path, index=False)\n",
        "    print(f\"Saved hateeval_gpt_ngoemotion dataset to : {file_path}\\n\")\n",
        "    return dataset_mistral\n",
        "if __name__ == \"__main__\":\n",
        "  dataset_mistral = main()\n"
      ],
      "metadata": {
        "id": "Ux1KAIvcyVD3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210,
          "referenced_widgets": [
            "49e27c57401d488fb3fb14991ca6e386",
            "4892aa43c30645dba05dc3d5ca4259b0",
            "e4de42ebf52747e6be1f9eafaba80fbd",
            "ddadb21308e448e4951f411cc31132b1",
            "4838e4c35d6046c5afdee6be43e2b5bf",
            "b7ee387cfabe47608c5c3773b5246408",
            "2847ff752e4442dbaac5f51b54a3cbfd",
            "88267298ee314c07aae871c0fce28c48",
            "1cd983545b8f45a8a600340c2450f9be",
            "9d8498a996aa4debbe845fcfec645819",
            "fc3d1773dd404ef7bb24f549b4568073"
          ]
        },
        "outputId": "12c1ac5e-aab0-4be5-9d47-a4e3e50f32d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "49e27c57401d488fb3fb14991ca6e386"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rReprocessing Unknown Emotions:   0%|          | 0/1 [33:35<?, ?it/s]\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'def main():\\n    llm_prompt = LLM_sentiment()\\n\\n    # dataset = hate_eval_ngoemotion[[\\'text\\', \\'hateeval_gpt40mini_ngoemotion_prompt\\']]\\n    # dataset_mistral = hate_eval_vanilla[[\\'text\\']]\\n    dataset_mistral = pd.read_csv(\\'/content/drive/MyDrive/PhD with Rawat/WOAH_2025/conan/conan_all_prompt_gpt_sent_emot.csv\\')\\n    # invalid_rows = dataset_mistral[(dataset_mistral[emotion_column] == \\'unknown\\')]\\n\\n    # for item in col_names:\\n    tqdm.pandas()\\n    dataset_mistral[\\'gpt_ngoemotion_sentiment_mistral\\'] = dataset[\\'hateeval_gpt40mini_ngoemotion_prompt\\'].progress_apply(lambda x: pd.Series(llm_prompt.sentiment_mistral_2(x)))\\n    dataset_mistral[\\'gpt_ngoemotion_emotion_mistral\\'] = dataset[\\'hateeval_gpt40mini_ngoemotion_prompt\\'].progress_apply(lambda x: pd.Series(llm_prompt.emotion_mistral_2(x)))\\n\\n    file_name = f\\'hateeval_gpt_ngoemotion_mistral_sentemot_df.csv\\'\\n    file_path = os.path.join(output_path , file_name)\\n    dataset_mistral.to_csv(file_path, index=False)\\n    print(f\"Saved hateeval_gpt_ngoemotion dataset to : {file_path}\\n\")\\n    return dataset_mistral\\nif __name__ == \"__main__\":\\n  dataset_mistral = main()'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9tJTXNgDQTq7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}